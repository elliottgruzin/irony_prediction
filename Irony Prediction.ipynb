{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irony Prediction\n",
    "\n",
    "This notebook is going to cover the main scripts I wrote. Other files used during this process are commented and in the repo, but are not going to be included in the script for sake of intelligibility.\n",
    "\n",
    "## Imports\n",
    "\n",
    "My first experiments use Bert as a Service, so make sure that is running in the background before running the following scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from benchmark script\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import logging\n",
    "import codecs\n",
    "\n",
    "## Additional packages used by Elliott Gruzin\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "from torch.utils.data import TensorDataset\n",
    "from dataset import Dataset\n",
    "from feedforward_network import LSTMNetwork\n",
    "from feedforward_network import Feedforward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from transformers import AutoTokenizer ## from hugging face\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, define method for parsing the dataset\n",
    "\n",
    "This is identical to the code used in the benchmark system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(fp):\n",
    "    '''\n",
    "    Loads the dataset .txt file with label-tweet on each line and parses the dataset.\n",
    "    :param fp: filepath of dataset\n",
    "    :return:\n",
    "        corpus: list of tweet strings of each tweet.\n",
    "        y: list of labels\n",
    "    '''\n",
    "    y = []\n",
    "    corpus = []\n",
    "    with open(fp, 'rt') as data_in:\n",
    "        for line in data_in:\n",
    "            if not line.lower().startswith(\"tweet index\"): # discard first line if it contains metadata\n",
    "                line = line.rstrip() # remove trailing whitespace\n",
    "                label = int(line.split(\"\\t\")[1])\n",
    "                tweet = line.split(\"\\t\")[2]\n",
    "                y.append(label)\n",
    "                corpus.append(tweet)\n",
    "\n",
    "    return corpus, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next define a process for creating embeddings and sentiment scores for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(corpus):\n",
    "    '''\n",
    "    Tokenizes and creates sentence vectors.\n",
    "    :param corpus: A list of strings each string representing document.\n",
    "    :return: X: List of BERT-embedded sentences, as well as retokenized corpus.\n",
    "    '''\n",
    "    \n",
    "    # Part 1: Compute Sentiment scores\n",
    "    \n",
    "    sentiments = []\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    for sentence in corpus:\n",
    "        sentiment_values = np.array(list(analyser.polarity_scores(sentence).values()))\n",
    "        sentiments.append(sentiment_values)\n",
    "        \n",
    "    # Part 2: Derive sentence embeddings\n",
    "        \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    new_corpus = [' '.join(tokenizer(sentence)) for sentence in corpus]\n",
    "    corpus = new_corpus\n",
    "    bc = BertClient()\n",
    "    X = bc.encode(corpus)\n",
    "    return X, sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now actually compute them and store them\n",
    "\n",
    "The method of storage here is crucial for the dataloading later in the process. To see how this is relevant, observe how the ids are chosen and later referenced in the dataset.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store BERT Embeddings\n",
    "\n",
    "# STEP 1: Make dictionaries to identify samples, and give label\n",
    "\n",
    "partition_dict = {'train':[],'test':[]}\n",
    "label_dict = {}\n",
    "\n",
    "# STEP 2: Compute and store\n",
    "\n",
    "for set in ['train','test']:\n",
    "\n",
    "    dataset = \"./{}_no_hashtag.txt\".format(set)\n",
    "    corpus, y = parse_dataset(dataset)\n",
    "    print('Extracting BERT embeddings for each tweet...')\n",
    "    \n",
    "    X, sentiments = featurize(corpus)\n",
    "    print('Embeddings extracted. Storing embeddings...')\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        \n",
    "        # Give each sentence an id, and link with irony label\n",
    "        \n",
    "        id = set+str(i)\n",
    "        partition_dict[set].append(id)\n",
    "        label_dict[id] = y[i]\n",
    "        sentence_and_sentiment = np.append(X[i],sentiments[i])\n",
    "        \n",
    "        # Save sentence and sentiment as a tensor\n",
    "        \n",
    "        embedding = torch.from_numpy(sentence_and_sentiment)\n",
    "        torch.save(embedding, 'data/dehashtagged/{}.pt'.format(id))\n",
    "    \n",
    "    print('Embeddings stored for the {} dataset.\\n'.format(set))\n",
    "\n",
    "print('Writing dictionaries...')\n",
    "\n",
    "p_dic = open('data/dehashtagged/partition_dict.pkl','wb')\n",
    "l_dic = open('data/dehashtagged/label_dict.pkl','wb')\n",
    "pickle.dump(partition_dict, p_dic)\n",
    "pickle.dump(label_dict, l_dic)\n",
    "p_dic.close()\n",
    "l_dic.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have sentence embeddings stored, we can use them to train our models\n",
    "\n",
    "### First use Cuda, and set some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Choosing parameters\n",
    "\n",
    "batch_size = 32\n",
    "layer_number = 3\n",
    "layer_width = 25\n",
    "lr8 = 0.001\n",
    "\n",
    "# Setting parameters\n",
    "train_params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "\n",
    "test_params = {'batch_size': 1,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 1}\n",
    "\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now make the training generators to be able to perform training in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jar = open('data/twitter_tokenizing_and_sentiment/partition_dict.pkl','rb')\n",
    "partition = pickle.load(jar)\n",
    "jar2 = open('data/twitter_tokenizing_and_sentiment/label_dict.pkl','rb')\n",
    "labels = pickle.load(jar2)\n",
    "jar.close()\n",
    "jar2.close()\n",
    "\n",
    "# Generators\n",
    "\n",
    "training_set = Dataset(partition['train'], labels,'twitter_tokenizing_and_sentiment')\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **train_params)\n",
    "\n",
    "test_set = Dataset(partition['test'], labels,'twitter_tokenizing_and_sentiment')\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **test_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I define my Feedforward network to take sentence embeddings and output vectors of size 1. \n",
    "\n",
    "I also define the test loss function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = Feedforward(772, layer_number, layer_width)\n",
    "ffn.cuda()\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(ffn.parameters(), lr=float(lr8), momentum=0.9)\n",
    "\n",
    "def test_loss():\n",
    "    'Calculates loss from model on test set'\n",
    "    ffn.eval()\n",
    "    test_loss = 0\n",
    "    for x, y in test_generator:\n",
    "\n",
    "        x = x.to(device=torch.device('cuda:0')).float()\n",
    "        y = y.to(device=torch.device('cuda:0'))\n",
    "        pred_output = ffn(x)\n",
    "        loss = criterion(pred_output.float(), y.unsqueeze(1).float())\n",
    "        test_loss += loss.data.cpu().numpy()\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below, we perform model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 0\n",
    "test_loss_list = []\n",
    "\n",
    "# Training begins\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('epoch {} done'.format(num_epochs))\n",
    "    ffn.train()\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x, y in training_generator:\n",
    "        \n",
    "        # Transfer to GPU\n",
    "        \n",
    "        x, y = x.to(device).float(), y.to(device)\n",
    "        y = y.unsqueeze(1)\n",
    "        \n",
    "        # Model computations\n",
    "        \n",
    "        pred_labels = ffn(x)\n",
    "        loss = criterion(pred_labels.float(), y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Get test loss\n",
    "    \n",
    "    t_loss = test_loss()\n",
    "    print('test loss: ', t_loss)\n",
    "    test_loss_list.append(t_loss)\n",
    "\n",
    "\n",
    "    # Early stop if current loss on the test set > loss from 10 epochs ago -- i.e. the model starts to overfit to the training data\n",
    "    \n",
    "    if num_epochs > 10:\n",
    "        test_loss_list.pop(0)\n",
    "        if t_loss > test_loss_list[0]:\n",
    "            break\n",
    "\n",
    "    num_epochs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now evaluate the model by pushing the test set through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output_list=[]\n",
    "labels=[]\n",
    "\n",
    "# push test set through one more time\n",
    "\n",
    "ffn.eval()\n",
    "for x, y in test_generator:\n",
    "    x = x.to(device=torch.device('cuda:0')).float()\n",
    "    y = y.to(device=torch.device('cuda:0'))\n",
    "    pred_output = ffn(x)\n",
    "    pred_output_list.append(pred_output.data.cpu().numpy()[0][0])\n",
    "    labels.append(y.data.cpu().numpy()[0])\n",
    "\n",
    "# generate predictions given models outputs, then compute f1\n",
    "\n",
    "pred_labels = [0 if output-0.5<0 else 1 for output in pred_output_list]\n",
    "print('\\n\\n\\n ################# RESULTS ##############\\n\\n')\n",
    "print(metrics.f1_score(labels, pred_labels, pos_label=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But this is just one method... What about lexical embeddings in an LSTM?\n",
    "\n",
    "## We need to define some new processes -- first, lets extract GloVe embeddings. The embeddings I use are the 100 dimensional embeddings trained on Twitter data downloaded from: http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_dict():\n",
    "\n",
    "    # make UNK vector (glove has none by default -- code taken from https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt/53717345#53717345)\n",
    "\n",
    "    with open('glove.twitter.27B.100d.txt','r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            pass\n",
    "    n_vec = i + 1\n",
    "    hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "    vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "    with open('glove.twitter.27B.100d.txt', 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "    average_vec = np.mean(vecs, axis=0)\n",
    "    word2embed = {}\n",
    "    word2embed['UNK'] = average_vec\n",
    "\n",
    "    # Now link every word with its embedding\n",
    "    \n",
    "    with open('glove.twitter.27B.100d.txt','r') as glove:\n",
    "        for line in glove:\n",
    "            sentence = line.split()\n",
    "            word = sentence[0]\n",
    "            vector = np.asarray(sentence[1:], 'float32')\n",
    "            word2embed[word] = vector\n",
    "\n",
    "    return word2embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We redefine how we 'featurize' the corpus\n",
    "\n",
    "Instead of making one sentence embedding, the sentence is stored as a series of lexical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(corpus, embed_dict):\n",
    "    '''\n",
    "    Tokenizes and creates sentence vectors.\n",
    "    :param corpus: A list of strings each string representing document.\n",
    "    :return: X: List of sentences, which are lists of embedded words.\n",
    "    '''\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    new_corpus = [' '.join(tokenizer(sentence)) for sentence in corpus]\n",
    "    corpus = new_corpus\n",
    "    X = []\n",
    "    for line in corpus:\n",
    "        sentence = []\n",
    "        for word in line:\n",
    "            try:\n",
    "                sentence.append(embed_dict[word])\n",
    "            except KeyError:\n",
    "                sentence.append(embed_dict['UNK'])\n",
    "        X.append(np.asarray(sentence))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now compute and store as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Extracting GloVe embedding dictionary...')\n",
    "\n",
    "word2embed = make_embedding_dict()\n",
    "\n",
    "print('Using embedding dictionary\\n\\n')\n",
    "\n",
    "# STEP 1: Make dictionaries to identify samples, and give label\n",
    "\n",
    "partition_dict = {'train':[],'test':[]}\n",
    "label_dict = {}\n",
    "\n",
    "# STEP 2: Compute and store\n",
    "\n",
    "for set in ['train','test']:\n",
    "\n",
    "    dataset = \"./{}_no_hashtag.txt\".format(set)\n",
    "    corpus, y = parse_dataset(dataset)\n",
    "    print('Extracting embeddings for each tweet...')\n",
    "    X = featurize(corpus, word2embed)\n",
    "    print('Embeddings extracted. Storing embeddings...')\n",
    "    for i in range(len(y)):\n",
    "        id = set+str(i)\n",
    "        partition_dict[set].append(id)\n",
    "        label_dict[id] = y[i]\n",
    "        embedding = torch.from_numpy(X[i])\n",
    "        torch.save(embedding, 'data/glove_lstm/{}.pt'.format(id))\n",
    "    print('Embeddings stored for the {} dataset.\\n'.format(set))\n",
    "\n",
    "print('Writing dictionaries...')\n",
    "\n",
    "p_dic = open('data/glove_lstm/partition_dict.pkl','wb')\n",
    "l_dic = open('data/glove_lstm/label_dict.pkl','wb')\n",
    "pickle.dump(partition_dict, p_dic)\n",
    "pickle.dump(label_dict, l_dic)\n",
    "p_dic.close()\n",
    "l_dic.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have the data (stored in a different directory) we can train an LSTM on it.\n",
    "\n",
    "## I reset some parameters below, and generate the appropriate dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing parameters\n",
    "\n",
    "batch_size = 1\n",
    "layer_number = 3\n",
    "layer_width = 50\n",
    "lr8 = 0.001\n",
    "\n",
    "# Setting parameters\n",
    "train_params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "\n",
    "test_params = {'batch_size': 1,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 1}\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "jar = open('data/glove_lstm/partition_dict.pkl','rb')\n",
    "partition = pickle.load(jar)\n",
    "jar2 = open('data/glove_lstm/label_dict.pkl','rb')\n",
    "labels = pickle.load(jar2)\n",
    "jar.close()\n",
    "jar2.close()\n",
    "\n",
    "# Generators\n",
    "\n",
    "training_set = Dataset(partition['train'], labels,'glove_lstm')\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **train_params)\n",
    "\n",
    "test_set = Dataset(partition['test'], labels,'glove_lstm')\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM and new test loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "lstm = LSTMNetwork(100, layer_number, layer_width)\n",
    "lstm.cuda()\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=float(lr8), momentum=0.9)\n",
    "\n",
    "def test_loss():\n",
    "    'Calculates loss from model on test set'\n",
    "    lstm.eval()\n",
    "    test_loss = 0\n",
    "    for x, y in test_generator:\n",
    "        lstm.zero_grad()\n",
    "        x = torch.transpose(x,0,1)\n",
    "        x = x.to(device=torch.device('cuda:0')).float()\n",
    "        y = y.to(device=torch.device('cuda:0'))\n",
    "        pred_output = lstm(x).squeeze(1)\n",
    "        pred_output = pred_output[-1,:]\n",
    "        loss = criterion(pred_output.float(), y.unsqueeze(1).float())\n",
    "        test_loss += loss.data.cpu().numpy()\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now train the new model\n",
    "\n",
    "Certain things are not ideal here. I did not have time to implement a batching system -- that would have involved padding. The resulting training takes very long compared to the feedforward system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 0\n",
    "test_loss_list = []\n",
    "\n",
    "# Training begins\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('epoch {} done'.format(num_epochs))\n",
    "    lstm.train()\n",
    "    num_batches = 0\n",
    "    for x, y in training_generator:\n",
    "\n",
    "        lstm.zero_grad()\n",
    "        # transpose such that sequence length comes first, batch second\n",
    "        x = torch.transpose(x,0,1)\n",
    "        num_batches += 1\n",
    "        # Transfer to GPU\n",
    "        x, y = x.to(device).float(), y.to(device)\n",
    "        # Model computations\n",
    "        pred_labels = lstm(x).squeeze(1)\n",
    "        pred_labels = pred_labels[-1,:]\n",
    "        loss = criterion(pred_labels.float(), y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # compute test loss\n",
    "    t_loss = test_loss()\n",
    "    print('test loss: ', t_loss)\n",
    "    test_loss_list.append(t_loss)\n",
    "\n",
    "\n",
    "    # Early stop just as before\n",
    "    if num_epochs > 10:\n",
    "        test_loss_list.pop(0)\n",
    "        if t_loss > test_loss_list[0]:\n",
    "            break\n",
    "\n",
    "    num_epochs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just as before, we can evaluate by pushing the test set through and computing an F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output_list=[]\n",
    "labels=[]\n",
    "\n",
    "lstm = torch.load('lstm.p')\n",
    "\n",
    "# push test set through one more time\n",
    "\n",
    "lstm.eval()\n",
    "for x, y in test_generator:\n",
    "    x = x.to(device=torch.device('cuda:0')).float().transpose(0,1)\n",
    "    y = y.to(device=torch.device('cuda:0'))\n",
    "    pred_output = lstm(x)[-1,:,:]\n",
    "    # print(pred_output)\n",
    "    # print(pred_output.cpu().detach().numpy()[0][0])\n",
    "    # exit()\n",
    "\n",
    "    pred_output_list.append(pred_output.cpu().detach().numpy()[0][0])\n",
    "    labels.append(y.cpu().numpy()[0])\n",
    "\n",
    "# generate predictions given models outputs, then compute f1\n",
    "\n",
    "pred_labels = [0 if output-0.5<0 else 1 for output in pred_output_list]\n",
    "print('\\n\\n\\n ################# RESULTS ##############\\n\\n')\n",
    "print(metrics.f1_score(labels, pred_labels, pos_label=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
